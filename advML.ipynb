{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import csv\n",
    "import itertools\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import zlib\n",
    "import importlib\n",
    "\n",
    "# Third-party imports\n",
    "import cv2\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import skimage\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from skimage.transform import resize\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import (KFold, StratifiedKFold, cross_val_score,\n",
    "                                     learning_curve, train_test_split)\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from tqdm import tqdm\n",
    "import foolbox as fb\n",
    "import tensorflow as tf\n",
    "import eagerpy as ep\n",
    "\n",
    "\n",
    "# Keras-specific imports\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import layers, models, optimizers\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import (Activation, AveragePooling2D, BatchNormalization,\n",
    "                          Conv2D, Dense, Dropout, Flatten, Lambda, MaxPool2D,\n",
    "                          MaxPooling2D)\n",
    "from keras.models import Model, Sequential, model_from_json\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mnorton/.pyenv/versions/3.11.3/lib/python3.11/site-packages/foolbox/models/tensorflow.py:13: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mnorton/.pyenv/versions/3.11.3/lib/python3.11/site-packages/foolbox/models/tensorflow.py:13: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('assets/models/ASL_DNN_3D.h5')\n",
    "fmodel = fb.TensorFlowModel(model, bounds=(0, 1), preprocessing=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m X_test_pca \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/mnorton/Desktop/College/Grad Victory Lap/CMSE 890/data/X_test_pca.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m y_test \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/mnorton/Desktop/College/Grad Victory Lap/CMSE 890/data/y_test.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m y_testHot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/mnorton/Desktop/College/Grad Victory Lap/CMSE 890/data/y_testHot.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/mnorton/Desktop/College/Grad Victory Lap/CMSE 890/data/X_test.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/numpy/lib/npyio.py:412\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    410\u001b[0m _ZIP_SUFFIX \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;130;01m\\x05\u001b[39;00m\u001b[38;5;130;01m\\x06\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# empty zip files start with this\u001b[39;00m\n\u001b[1;32m    411\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX)\n\u001b[0;32m--> 412\u001b[0m magic \u001b[38;5;241m=\u001b[39m \u001b[43mfid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# If the file size is less than N, we need to make sure not\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# to seek past the beginning of the file\u001b[39;00m\n\u001b[1;32m    415\u001b[0m fid\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mmin\u001b[39m(N, \u001b[38;5;28mlen\u001b[39m(magic)), \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# back-up\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_test_pca = np.load(\"/Users/mnorton/Desktop/College/Grad Victory Lap/CMSE 890/data/X_test_pca.npy\")\n",
    "y_test = np.load(\"/Users/mnorton/Desktop/College/Grad Victory Lap/CMSE 890/data/y_test.npy\")\n",
    "y_testHot = np.load(\"/Users/mnorton/Desktop/College/Grad Victory Lap/CMSE 890/data/y_testHot.npy\")\n",
    "X_test = np.load(\"/Users/mnorton/Desktop/College/Grad Victory Lap/CMSE 890/data/X_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"delete\", \"nothing\", \"space\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from foolbox.attacks import L2PGD\n",
    "\n",
    "attack = L2PGD()\n",
    "\n",
    "# Choose an image and label from your test set\n",
    "image, label = X_test[0], y_test[0]\n",
    "#image = ep.astensor(np.expand_dims(image, axis=0)) \n",
    "\n",
    "label = np.array([label])\n",
    "label = ep.astensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilons = np.linspace(0.0, 0.005, num=20)\n",
    "\n",
    "# # Apply the attack\n",
    "# adversarial_image, adversarial_label, success = attack(fmodel, image, criterion=fb.criteria.Misclassification(label), epsilons=epsilons)\n",
    "\n",
    "# # Check if the attack was successful\n",
    "# if success:\n",
    "#     print(\"Attack successful!\")\n",
    "# else:\n",
    "#     print(\"Attack failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap your model into a Foolbox model\n",
    "fmodel = fb.TensorFlowModel(model, bounds=(0, 1), preprocessing=None)\n",
    "\n",
    "# Choose an attack method, e.g., FGSM (Fast Gradient Sign Method)\n",
    "attack = fb.attacks.FGSM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare an example input image and its correct label\n",
    "import numpy as np\n",
    "image = np.random.rand(1, 50, 50, 3).astype(np.float32) # Example image\n",
    "image = tf.convert_to_tensor(image, dtype=tf.float32)  # Convert to TensorFlow tensor\n",
    "\n",
    "label = np.array([3]) # Example true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = X_test\n",
    "#images_expanded = images.reshape((-1, 1, 50, 50, 3))\n",
    "\n",
    "mages = tf.convert_to_tensor(images, dtype=tf.float32)  # Convert to TensorFlow tensor\n",
    "\n",
    "labels = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_images(batch_idx, original_images, adversarial_images, labels, n=5):\n",
    "    num_images_to_plot = min(n, len(original_images))\n",
    "    plt.figure(figsize=(10, 6))  # Adjusted for three rows of images\n",
    "    \n",
    "    for i in range(num_images_to_plot):\n",
    "        # Plot original images\n",
    "        plt.subplot(3, num_images_to_plot, i + 1)\n",
    "        plt.imshow(original_images[i])\n",
    "        plt.title(f\"Orig: {labels[i]}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Calculate noise/difference and amplify for visualization\n",
    "        noise = adversarial_images[i] - original_images[i]\n",
    "        # Normalize the noise to [0, 1] for visualization\n",
    "        noise_normalized = (noise - noise.min()) / (noise.max() - noise.min())\n",
    "        plt.subplot(3, num_images_to_plot, num_images_to_plot + i + 1)\n",
    "        plt.imshow(noise_normalized, cmap='viridis')  # Using a colormap to enhance visibility\n",
    "        plt.title(\"Amplified Noise\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Plot adversarial images\n",
    "        plt.subplot(3, num_images_to_plot, 2 * num_images_to_plot + i + 1)\n",
    "        adv_img = adversarial_images[i] if adversarial_images.ndim == 4 else adversarial_images\n",
    "        plt.imshow(adv_img)\n",
    "        plt.title(\"Adv\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=keras.optimizers.Adam(), \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a batch_size defined\n",
    "batch_size = 50\n",
    "original_confidences = []\n",
    "adv_confidences = []\n",
    "num_epochs = 5\n",
    "\n",
    "j = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch = images[i:i+batch_size]\n",
    "        batch_labels = labels[i:i + batch_size]\n",
    "        batch_tensor = tf.convert_to_tensor(batch, dtype=tf.float32)\n",
    "\n",
    "        # Generate adversarial examples\n",
    "        criterion = fb.criteria.Misclassification(labels=batch_labels)\n",
    "        epsilons = [0.01, 0.1, 0.15, 0.2]  # Example epsilon values\n",
    "        adversarials, _, success = attack(fmodel, inputs=batch_tensor, criterion=criterion, epsilons=epsilons)\n",
    "\n",
    "        print(f\"Batch {i//batch_size} success rate:\", success.numpy().mean())\n",
    "\n",
    "        # Convert tensors to NumPy arrays for plotting\n",
    "        adversarials_np = np.array([adv.numpy() for adv in adversarials]) if isinstance(adversarials, list) else adversarials.numpy()\n",
    "        # Convert adversarials to TensorFlow tensor for model prediction\n",
    "        adversarials_tensor = tf.stack(adversarials, axis=0)\n",
    "        \n",
    "        adversarials_tensor = tf.reshape(adversarials_tensor, (-1, 50, 50, 3))\n",
    "\n",
    "        # Get model predictions for both original and adversarial images\n",
    "        original_predictions = fmodel(batch_tensor).numpy()\n",
    "        adv_predictions = fmodel(adversarials_tensor).numpy()\n",
    "\n",
    "        # Extract the confidence scores for the true class (or top paredicted class)\n",
    "        original_confidences.extend(np.max(original_predictions, axis=1))\n",
    "        adv_confidences.extend(np.max(adv_predictions, axis=1))\n",
    "\n",
    "        n=5\n",
    "        # Select a subset for plotting\n",
    "        selected_batch = batch[:n]  # Adjust the slice as necessary\n",
    "        selected_adversarials = adversarials[0].numpy()[:n]    \n",
    "        selected_labels = batch_labels[:n]  # Adjust the slice as necessary\n",
    "\n",
    "        # # if j <= 3:\n",
    "        # #     # Plotting images from the current batch\n",
    "        # #     plot_images(i//batch_size, selected_batch, selected_adversarials, selected_labels, n=5)\n",
    "        # #     j+=1\n",
    "        \n",
    "        # Combine original and adversarial examples\n",
    "        combined_images = tf.concat([batch_tensor, adversarials_tensor], axis=0)\n",
    "        # Assuming the labels for adversarials should match the original labels, you'll need to replicate the labels\n",
    "        replicated_labels = tf.repeat(batch_labels, repeats=tf.shape(adversarials)[0], axis=0)\n",
    "\n",
    "        # Combine the original labels and the replicated labels for adversarials\n",
    "        combined_labels = tf.concat([batch_labels, replicated_labels], axis=0)\n",
    "        \n",
    "        # Shuffle the combined dataset\n",
    "        idx = tf.random.shuffle(tf.range(combined_images.shape[0]))\n",
    "        combined_images = tf.gather(combined_images, idx)\n",
    "        combined_labels = tf.gather(combined_labels, idx)\n",
    "\n",
    "        # Assuming 'num_classes' is the number of classes in your dataset\n",
    "        num_classes = 30  # Adjust this to match your actual number of classes\n",
    "\n",
    "        # One-hot encode the labels\n",
    "        combined_labels_one_hot = to_categorical(combined_labels, num_classes=num_classes)\n",
    "\n",
    "        # Now use the one-hot encoded labels for training\n",
    "        model.train_on_batch(combined_images, combined_labels_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"assets/models/ASL_Adv_FD.h5\")\n",
    "model.save(\"assets/models/ASL_Adv_FD.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming test_images and test_labels are your regular test examples and their true labels\n",
    "loss, accuracy = model.evaluate(X_test, y_testHot, verbose=0)\n",
    "print(f\"Accuracy on regular examples: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#batch_size = 500\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Select a batch of test images and labels\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m test_batch \u001b[38;5;241m=\u001b[39m \u001b[43mX_test\u001b[49m  \u001b[38;5;66;03m# Assuming test_images is defined\u001b[39;00m\n\u001b[1;32m      4\u001b[0m test_batch_labels \u001b[38;5;241m=\u001b[39m y_test  \u001b[38;5;66;03m# Assuming test_labels is defined\u001b[39;00m\n\u001b[1;32m      6\u001b[0m test_batch_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(test_batch, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "#batch_size = 500\n",
    "# Select a batch of test images and labels\n",
    "test_batch = X_test  # Assuming test_images is defined\n",
    "test_batch_labels = y_test  # Assuming test_labels is defined\n",
    "\n",
    "test_batch_tensor = tf.convert_to_tensor(test_batch, dtype=tf.float32)\n",
    "\n",
    "# Generate adversarial examples for the test batch\n",
    "criterion = fb.criteria.Misclassification(labels=test_batch_labels)\n",
    "epsilons = [0.01, 0.1, 0.15, 0.2]  # Example epsilon values\n",
    "adv_test_images, _, _ = attack(fmodel, inputs=test_batch_tensor, criterion=criterion, epsilons=epsilons)\n",
    "\n",
    "# Convert adversarial test images to the desired format (e.g., NumPy array)\n",
    "adv_test_images_np = np.array([adv.numpy() for adv in adv_test_images]) if isinstance(adv_test_images, list) else adv_test_images.numpy()\n",
    "\n",
    "# adv_test_labels would be the same as test_batch_labels since the labels do not change\n",
    "adv_test_labels = test_batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_test_images_np_reshaped = np.reshape(adv_test_images_np, (-1, 50, 50, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 50, 50, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_test_images_np_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, adv_accuracy = model.evaluate(adv_test_images_np, adv_test_labels, verbose=0)\n",
    "print(f\"Accuracy on adversarial examples: {adv_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a batch_size defined\n",
    "batch_size = 50\n",
    "original_confidences = []\n",
    "adv_confidences = []\n",
    "for i in range(0, len(images), batch_size):\n",
    "    batch = images[i:i+batch_size]\n",
    "    batch_labels = labels[i:i + batch_size]\n",
    "    batch_tensor = tf.convert_to_tensor(batch, dtype=tf.float32)\n",
    "    \n",
    "    criterion = fb.criteria.Misclassification(labels=batch_labels)\n",
    "\n",
    "    # Apply the attack\n",
    "    epsilons = [0.01, 0.1, 0.15, 0.2]\n",
    "    adversarials, _, success = attack(fmodel, inputs=batch_tensor, criterion=criterion, epsilons=epsilons)\n",
    "\n",
    "    # Display success rate for the batch\n",
    "    print(f\"Batch {i//batch_size} success rate:\", success.numpy().mean())\n",
    "\n",
    "    # Convert tensors to NumPy arrays for plotting\n",
    "    adversarials_np = np.array([adv.numpy() for adv in adversarials]) if isinstance(adversarials, list) else adversarials.numpy()\n",
    "    # Convert adversarials to TensorFlow tensor for model prediction\n",
    "    adversarials_tensor = tf.stack(adversarials, axis=0)\n",
    "    \n",
    "    adversarials_tensor = tf.reshape(adversarials_tensor, (-1, 50, 50, 3))\n",
    "\n",
    "    # Get model predictions for both original and adversarial images\n",
    "    original_predictions = fmodel(batch_tensor).numpy()\n",
    "    adv_predictions = fmodel(adversarials_tensor).numpy()\n",
    "\n",
    "    # Extract the confidence scores for the true class (or top paredicted class)\n",
    "    original_confidences.extend(np.max(original_predictions, axis=1))\n",
    "    adv_confidences.extend(np.max(adv_predictions, axis=1))\n",
    "\n",
    "    n=5\n",
    "    # Select a subset for plotting\n",
    "    selected_batch = batch[:n]  # Adjust the slice as necessary\n",
    "    selected_adversarials = adversarials[0].numpy()[:n]    \n",
    "    selected_labels = batch_labels[:n]  # Adjust the slice as necessary\n",
    "\n",
    "    # Plotting images from the current batch\n",
    "    plot_images(i//batch_size, selected_batch, selected_adversarials, selected_labels, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = range(len(original_confidences))  # Adjust to match the length of adv_confidences\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(indices, original_confidences, label='Original Confidence')  # Match lengths\n",
    "plt.plot(indices, adv_confidences[:len(indices)], label='Adversarial Confidence', linestyle='--')\n",
    "plt.xlabel('Image Index')\n",
    "plt.ylabel('Confidence Score')\n",
    "plt.title('Confidence Score Changes Before and After Adversarial Attack')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select an image and its adversarial counterpart\n",
    "original_image = images[0]  # Replace with the correct index or method to select an image\n",
    "adv_image = adversarials[0]  # Replace with the correct index or method to select the corresponding adversarial image\n",
    "\n",
    "# Assuming original_image is a 2D array with shape (height, width, channels)\n",
    "height, width, channels = original_image.shape\n",
    "\n",
    "# Select two pixels to vary, for example the center and a corner\n",
    "pixel_indices = [(height // 2, width // 2), (0, 0)]  # Center and top-left corner\n",
    "\n",
    "# Generate a grid of values to iterate over for the two selected pixels\n",
    "value_range = np.linspace(0, 255, num=50)  # Example for pixel intensity range\n",
    "decision_surface = np.zeros((len(value_range), len(value_range)))\n",
    "\n",
    "# Generate the decision surface\n",
    "for i, val1 in enumerate(value_range):\n",
    "    for j, val2 in enumerate(value_range):\n",
    "        # Create a copy of the original image\n",
    "        perturbed_image = np.copy(original_image)\n",
    "        \n",
    "        # Modify the two selected pixels\n",
    "        perturbed_image[pixel_indices[0]] = val1\n",
    "        perturbed_image[pixel_indices[1]] = val2\n",
    "        \n",
    "        # Add your model prediction code here\n",
    "        # For example, assuming you have a function model_predict() to get predictions:\n",
    "        # decision_surface[i, j] = model_predict(perturbed_image)\n",
    "\n",
    "# The following lines are placeholders to create a fake decision surface for visualization\n",
    "# Replace with your model's prediction results\n",
    "decision_surface = np.sin(value_range)[:, None] * np.cos(value_range)\n",
    "\n",
    "# Plotting\n",
    "plt.contourf(value_range, value_range, decision_surface, cmap=plt.cm.RdYlBu)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Pixel 1 intensity')\n",
    "plt.ylabel('Pixel 2 intensity')\n",
    "plt.title('Decision Surface Visualization around Original and Adversarial Images')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure that the model is set to trainable to compute gradients\n",
    "model.trainable = True\n",
    "\n",
    "# Select the image for which you want to create the saliency map\n",
    "image = images[0]  # Ensure this image is preprocessed (if necessary) as per model requirements\n",
    "\n",
    "# Add a batch dimension and preprocess the image if required\n",
    "image_tensor = tf.expand_dims(image, axis=0)\n",
    "\n",
    "# Record operations for automatic differentiation\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(image_tensor)\n",
    "    # Forward pass through the model to get the logits\n",
    "    predictions = model(image_tensor)\n",
    "    \n",
    "    # Use the class with the highest probability\n",
    "    class_idx = tf.argmax(predictions[0])\n",
    "    class_logits = predictions[0][class_idx]\n",
    "\n",
    "# Calculate the gradients of the class logits with respect to the input image\n",
    "gradients = tape.gradient(class_logits, image_tensor)\n",
    "\n",
    "# Check if gradients are None\n",
    "if gradients is None:\n",
    "    raise ValueError(\"Failed to compute gradients.\")\n",
    "\n",
    "# Reduce the batch dimension and extract the maximum across the channels to get the saliency map\n",
    "saliency_map = tf.reduce_max(tf.abs(gradients), axis=-1)[0]\n",
    "\n",
    "# Plot the saliency map\n",
    "plt.imshow(saliency_map, cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.title('Saliency Map')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Your pre-trained model and original image\n",
    "original_image = images[0]  # Replace with the correct image\n",
    "\n",
    "# Function to apply perturbations\n",
    "def apply_perturbation(x, y, intensity):\n",
    "    perturbed_image = np.copy(original_image)\n",
    "    # Assuming the image is grayscale; if it's color, you'll need to modify all channels\n",
    "    perturbed_image[y, x] = np.clip(perturbed_image[y, x] + intensity, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "# Initialize the output widget\n",
    "output = widgets.Output()\n",
    "\n",
    "# Create sliders\n",
    "perturb_x_slider = widgets.IntSlider(min=0, max=original_image.shape[1] - 1, step=1, description='Perturb X')\n",
    "perturb_y_slider = widgets.IntSlider(min=0, max=original_image.shape[0] - 1, step=1, description='Perturb Y')\n",
    "intensity_slider = widgets.FloatSlider(min=0, max=1, step=0.01, description='Intensity')\n",
    "\n",
    "# Update function\n",
    "def update_image(change):\n",
    "    with output:\n",
    "        # Clear the current output\n",
    "        clear_output(wait=True)\n",
    "        # Apply perturbation\n",
    "        new_image = apply_perturbation(perturb_x_slider.value, perturb_y_slider.value, intensity_slider.value)\n",
    "        # Display the new image\n",
    "        plt.imshow(new_image, cmap='gray', vmin=0, vmax=1)\n",
    "        plt.title('Perturbed Image')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Observe changes in the slider values\n",
    "perturb_x_slider.observe(update_image, names='value')\n",
    "perturb_y_slider.observe(update_image, names='value')\n",
    "intensity_slider.observe(update_image, names='value')\n",
    "\n",
    "# Display everything\n",
    "display(perturb_x_slider, perturb_y_slider, intensity_slider)\n",
    "# Display the initial image\n",
    "update_image(None)  # Initial call to display the original image\n",
    "display(output)  # This is where the updated images will be displayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
